{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from convokit import Corpus, Speaker, Utterance\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the directory with where your downloaded cornell movie dialogs corpus is saved\n",
    "data_dir = \"C:\\\\Users\\\\Vinay\\\\OneDrive\\\\Documents\\\\GitHub\\\\Coordination\\\\ijcnlp_dailydialog\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"dialogues_text.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    dialog_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13118/13118 [00:00<00:00, 253076.61it/s]\n"
     ]
    }
   ],
   "source": [
    "utterance_data=[]\n",
    "count=0\n",
    "for utterance in tqdm(dialog_data):\n",
    "    \n",
    "    utterance_data.append( [info.strip( ) for info in utterance.split(\" __eou__\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(utterance_data)):\n",
    "    utterance_data[i].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding utterance index for a dialog\n",
    "x=0\n",
    "for i in range(len(utterance_data)):\n",
    "    for j in range(len(utterance_data[i])):\n",
    "        utterance_data[i][j]+=\" __alp__ \"+(str(j+x))\n",
    "    x+=len(utterance_data[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"dialogues_act.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    dialog_act = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13118/13118 [00:00<00:00, 416553.46it/s]\n"
     ]
    }
   ],
   "source": [
    "utterance_act=[]\n",
    "for utterance in tqdm(dialog_act):\n",
    "    \n",
    "    utterance_act.append([info.strip() for info in utterance.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(utterance_data)):\n",
    "    for j in range(len(utterance_data[i])):\n",
    "        utterance_data[i][j]+=\" __alp__ \"+utterance_act[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"dialogues_emotion.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    dialog_emotion = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13118/13118 [00:00<00:00, 340851.18it/s]\n"
     ]
    }
   ],
   "source": [
    "utterance_emotion=[]\n",
    "for utterance in tqdm(dialog_emotion):\n",
    "    \n",
    "    utterance_emotion .append( [info.strip() for info in utterance.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emotion index\n",
    "for i in range(len(utterance_data)):\n",
    "    for j in range(len(utterance_data[i])):\n",
    "        utterance_data[i][j]+=\" __alp__ \"+utterance_emotion[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So Dick , how about getting some coffee for tonight ? __alp__ 2 __alp__ 3 __alp__ 4',\n",
       " 'Coffee ? I don ’ t honestly like that kind of stuff . __alp__ 3 __alp__ 4 __alp__ 2',\n",
       " 'Come on , you can at least try a little , besides your cigarette . __alp__ 4 __alp__ 3 __alp__ 0',\n",
       " 'What ’ s wrong with that ? Cigarette is the thing I go crazy for . __alp__ 5 __alp__ 1 __alp__ 1',\n",
       " 'Not for me , Dick . __alp__ 6 __alp__ 1 __alp__ 0']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So Dick , how about getting some coffee for tonight ? __alp__ 2 __alp__ 3 __alp__ 4 __alp__ 2',\n",
       " 'Coffee ? I don ’ t honestly like that kind of stuff . __alp__ 3 __alp__ 4 __alp__ 2 __alp__ 3',\n",
       " 'Come on , you can at least try a little , besides your cigarette . __alp__ 4 __alp__ 3 __alp__ 0 __alp__ 2',\n",
       " 'What ’ s wrong with that ? Cigarette is the thing I go crazy for . __alp__ 5 __alp__ 1 __alp__ 1 __alp__ 3',\n",
       " 'Not for me , Dick . __alp__ 6 __alp__ 1 __alp__ 0 __alp__ 2']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding speakers\n",
    "#we will assume speaker 0 and 1 speaking alternatively\n",
    "for i in range(len(utterance_data)):\n",
    "    for j in range(len(utterance_data[i])):\n",
    "        utterance_data[i][j]+=\" __alp__ \"+str(2*i+j%2)\n",
    "utterance_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversation id\n",
    "for i in range(len(utterance_data)):\n",
    "    for j in range(len(utterance_data[i])):\n",
    "        utterance_data[i][j]+=\" __alp__ \"+str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So Dick , how about getting some coffee for tonight ? __alp__ 2 __alp__ 3 __alp__ 4 __alp__ 2 __alp__ 1',\n",
       " 'Coffee ? I don ’ t honestly like that kind of stuff . __alp__ 3 __alp__ 4 __alp__ 2 __alp__ 3 __alp__ 1',\n",
       " 'Come on , you can at least try a little , besides your cigarette . __alp__ 4 __alp__ 3 __alp__ 0 __alp__ 2 __alp__ 1',\n",
       " 'What ’ s wrong with that ? Cigarette is the thing I go crazy for . __alp__ 5 __alp__ 1 __alp__ 1 __alp__ 3 __alp__ 1',\n",
       " 'Not for me , Dick . __alp__ 6 __alp__ 1 __alp__ 0 __alp__ 2 __alp__ 1']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ok , I'll get this rate right away . __alp__ 102978 __alp__ 4 __alp__ 0 __alp__ 26235 __alp__ 13117\""
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_data_new=[]\n",
    "for xs in utterance_data:\n",
    "    for x in xs:\n",
    "        utterance_data_new.append(x)\n",
    "utterance_data_new[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_meta={}\n",
    "for i in range(26236):\n",
    "    speaker_meta[str(i)]={\"speaker_id\": i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_speakers = {k: Speaker(id = k, meta = v) for k,v in speaker_meta.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102980/102980 [00:01<00:00, 72753.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of utterances = 102980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "utterance_corpus = {}\n",
    "count = 0\n",
    "for utterance in tqdm(utterance_data_new):\n",
    "    \n",
    "    utterance_info = [info.strip() for info in utterance.split(\" __alp__\")]\n",
    "    \n",
    "    if len(utterance_info) < 4:\n",
    "        print(utterance_info)\n",
    "        \n",
    "    try:\n",
    "        text, utt_idx, utt_act, utt_emo,speaker_id,conver_id = utterance_info[0], utterance_info[1], utterance_info[2], int(utterance_info[3]),utterance_info[4],utterance_info[5]\n",
    "    except:\n",
    "        print(utterance_info)\n",
    "    \n",
    "    meta = {'utt_emo': utt_emo, 'utt_idx': utt_idx,'utt_act': utt_act}\n",
    "    \n",
    "    # root & reply_to will be updated later, timestamp is not applicable \n",
    "    utterance_corpus[utt_idx] = Utterance(id=utt_idx, speaker= corpus_speakers[speaker_id], text=text, conversation_id=conver_id, meta= meta)\n",
    "\n",
    "print(\"Total number of utterances = {}\".format(len(utterance_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'obj_type': 'utterance', 'meta': {'utt_emo': 2, 'utt_idx': '0', 'utt_act': '3'}, 'vectors': [], 'speaker': Speaker({'obj_type': 'speaker', 'meta': {'speaker_id': 0}, 'vectors': [], 'owner': None, 'id': '0'}), 'conversation_id': '0', 'reply_to': None, 'timestamp': None, 'text': 'The kitchen stinks .', 'owner': None, 'id': '0'})"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_corpus['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102980"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(utterance_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_id_definer=0\n",
    "for i in range(len(utterance_corpus)):\n",
    "    if i==0 or utterance_corpus[str(i)].conversation_id!=utterance_corpus[str(i-1)].conversation_id:\n",
    "        utterance_corpus[str(i)].reply_to=None\n",
    "    else:\n",
    "        utterance_corpus[str(i)].reply_to=str(i-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance(id: '20', conversation_id: 5, reply-to: 19, speaker: Speaker(id: 10, vectors: [], meta: {'speaker_id': 10}), timestamp: None, text: 'What ’ s wrong ? Didn ’ t you think it was fun ? !', vectors: [], meta: {'utt_emo': 6, 'utt_idx': '20', 'utt_act': '2'})\n",
      "10\n",
      "{'speaker_id': 21}\n"
     ]
    }
   ],
   "source": [
    "print(utterance_corpus['20'])\n",
    "print(utterance_corpus['50'].conversation_id)\n",
    "print(utterance_corpus['50'].speaker.meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "utternace_list=utterance_corpus.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordination_corpus=Corpus(utterances=utternace_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations in the dataset = 13118\n"
     ]
    }
   ],
   "source": [
    "print(\"number of conversations in the dataset = {}\".format(len(coordination_corpus.get_conversation_ids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample conversation 0:\n",
      "['0', '1']\n",
      "sample conversation 1:\n",
      "['2', '3', '4', '5', '6']\n",
      "sample conversation 2:\n",
      "['7', '8', '9', '10']\n",
      "sample conversation 3:\n",
      "['11', '12', '13', '14']\n"
     ]
    }
   ],
   "source": [
    "convo_ids = coordination_corpus.get_conversation_ids()\n",
    "for i, convo_idx in enumerate(convo_ids[0:4]):\n",
    "    print(\"sample conversation {}:\".format(i))\n",
    "    print(coordination_corpus.get_conversation(convo_idx).get_utterance_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordination_corpus.dump(\"coordination-corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import meta_index\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterances-index': {'utt_emo': [\"<class 'int'>\"],\n",
       "  'utt_idx': [\"<class 'str'>\"],\n",
       "  'utt_act': [\"<class 'str'>\"]},\n",
       " 'speakers-index': {'speaker_id': [\"<class 'int'>\"]},\n",
       " 'conversations-index': {},\n",
       " 'overall-index': {},\n",
       " 'version': 1,\n",
       " 'vectors': []}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_index(filename = os.path.join(os.path.expanduser(\"~\"), \".convokit/saved-corpora/coordination-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import convokit\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= Corpus(utterances=utternace_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<convokit.model.corpus.Corpus object at 0x000001C117046B50>\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x1c117046b50>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord= convokit.Coordination()\n",
    "coord.fit(corpus)\n",
    "coord.transform(corpus)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_of_emotions={}\n",
    "list_of_emotion=[]\n",
    "for i in range(len(utterance_corpus)):\n",
    "    emotion_key=utterance_corpus[str(i)].speaker.meta['speaker_id']\n",
    "    emotion_dict=utterance_corpus[str(i)].meta['utt_emo']\n",
    "    if emotion_key in dictionary_of_emotions:\n",
    "        dictionary_of_emotions[emotion_key].append(emotion_dict)\n",
    "    else: \n",
    "        dictionary_of_emotions[emotion_key]=[emotion_dict]\n",
    "for i in range(26236):\n",
    "    list_of_emotion.append(dictionary_of_emotions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= not equal\n",
    "#final=equal\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "t=set()\n",
    "q=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "        t.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (((emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0))):\n",
    "        #t.discard(int(speakera.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]!=emotion_speaker2[0] and emotion_speaker1[length1-1]==emotion_speaker2[length2-1]:\n",
    "            q.add(int(speakera.id))\n",
    "        #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (((emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0))):\n",
    "            #q.discard(int(speakera.id))\n",
    "len(t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "115 0.4\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= not equal\n",
    "#final= equal\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "        t.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "   # if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #t.discard(int(speakerb.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]!=emotion_speaker2[0] and emotion_speaker1[length1-1]==emotion_speaker2[length2-1]:\n",
    "            q.add(int(speakerb.id))\n",
    "        #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "            #q.discard(int(speakerb.id))\n",
    "print(len(t),len(q)/len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers whose initial emotions are not equal are  40.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers whose initial emotions are not equal are \",100*len(q)/len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= equal\n",
    "#final= equal\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "w=set()\n",
    "e=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "        w.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]==emotion_speaker2[0]) or (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #w.discard(int(speakera.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]==emotion_speaker2[0] and emotion_speaker1[length1-1]==emotion_speaker2[length2-1]:\n",
    "            e.add(int(speakera.id))\n",
    "        #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "            #e.discard(int(speakera.id))\n",
    "len(w)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "651 0.6605222734254992\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= equal\n",
    "#final= equal\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "        w.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #w.discard(int(speakerb.id))\n",
    "\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]==emotion_speaker2[0] and emotion_speaker1[length1-1]==emotion_speaker2[length2-1]:\n",
    "            e.add(int(speakerb.id))\n",
    "        #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "            #e.discard(int(speakerb.id))\n",
    "print(len(w),len(e)/len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers whose initial emotions are also same are  66.05222734254993\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers whose initial emotions are also same are \",100*len(e)/len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= not equal\n",
    "#final= similar\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "similar_t=set()\n",
    "similar_q=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "        similar_t.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #similar_t.discard(int(speakera.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "            if emotion_speaker1[length1-1]==emotion_speaker2[length2-1] or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==4 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==4) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==3) :\n",
    "                similar_q.add(int(speakera.id))\n",
    "            #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "                #similar_q.discard(int(speakera.id))\n",
    "len(similar_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "115 0.41739130434782606\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= not equal\n",
    "#final= similar\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "        similar_t.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #similar_t.discard(int(speakerb.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "            if emotion_speaker1[length1-1]==emotion_speaker2[length2-1] or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==4 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==4) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==3)  :\n",
    "                similar_q.add(int(speakerb.id))\n",
    "            #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "                #similar_q.discard(int(speakerb.id))\n",
    "print(len(similar_t),len(similar_q)/len(similar_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers is similar whose initial emotions are not same are are  41.73913043478261\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers is similar whose initial emotions are not same are are \",100*len(similar_q)/len(similar_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= equal\n",
    "#final= similar\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "similar_t_equal=set()\n",
    "similar_q_equal=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "        similar_t_equal.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #similar_t_equal.discard(int(speakera.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "            if emotion_speaker1[length1-1]==emotion_speaker2[length2-1] or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==4 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==4) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==3) :\n",
    "                similar_q_equal.add(int(speakera.id))\n",
    "            #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "                #similar_q_equal.discard(int(speakera.id))\n",
    "len(similar_t_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "651 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= equal\n",
    "#final= similar\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "        similar_t_equal.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #similar_t_equal.discard(int(speakerb.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "            if emotion_speaker1[length1-1]==emotion_speaker2[length2-1] or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==4 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==4) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==3)  :\n",
    "                similar_q_equal.add(int(speakerb.id))\n",
    "            #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "                #similar_q_equal.discard(int(speakerb.id))\n",
    "print(len(similar_t_equal),len(similar_q_equal)/len(similar_t_equal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers is similar whose initial emotions are also same are are  66.66666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers is similar whose initial emotions are also same are are \",100*len(similar_q_equal)/len(similar_t_equal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= any emotion\n",
    "#final= negative emotion of any one speaker\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "hypo2_t=set()\n",
    "hypo2_q=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=False):\n",
    "    if(round(score,5)==-0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    hypo2_t.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    if length1>1:\n",
    "        if ((emotion_speaker1[length1-1]==0) or (emotion_speaker1[length1-1]==1) or (emotion_speaker1[length1-1]==2) or (emotion_speaker1[length1-1]==3) or (emotion_speaker1[length1-1]==5)) and ((emotion_speaker2[length2-1]==0) or emotion_speaker2[length2-1]==1 or emotion_speaker2[length2-1]==2 or emotion_speaker2[length2-1]==3 or emotion_speaker2[length2-1]==5):\n",
    "            hypo2_q.add(int(speakera.id))\n",
    "len(hypo2_t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "496 0.5705645161290323\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= anything\n",
    "#final= negative emotion of any one speaker\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=False):\n",
    "    if(round(score,5)==-0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    hypo2_t.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    if length1>1:\n",
    "        if (emotion_speaker1[length1-1]==0 or emotion_speaker1[length1-1]==1 or emotion_speaker1[length1-1]==2 or emotion_speaker1[length1-1]==3 or emotion_speaker1[length1-1]==5) and (emotion_speaker2[length2-1]==0 or emotion_speaker2[length2-1]==1 or emotion_speaker2[length2-1]==2 or emotion_speaker2[length2-1]==3 or emotion_speaker2[length2-1]==5):\n",
    "            hypo2_q.add(int(speakerb.id))\n",
    "print(len(hypo2_t),len(hypo2_q)/len(hypo2_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of both 2 speakers is negative or no emotion are  57.056451612903224\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of both 2 speakers is negative or no emotion are \",100*len(hypo2_q)/len(hypo2_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= not negative emotion for both speakers\n",
    "#final= negative emotion of any one speaker\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "hypo2_t_ini=set()\n",
    "hypo2_q_ini=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=False):\n",
    "    if(round(score,5)==-0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    if (emotion_speaker1[0]==0 or emotion_speaker1[0]==4 or emotion_speaker1[0]==6) and (emotion_speaker2[0]==0 or emotion_speaker2[0]==4 or emotion_speaker2[0]==6):\n",
    "        hypo2_t_ini.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    if length1>1:\n",
    "        if ((emotion_speaker1[0]==0 or emotion_speaker1[0]==4 or emotion_speaker1[0]==6) and (emotion_speaker2[0]==0 or emotion_speaker2[0]==4 or emotion_speaker2[0]==6)) and (((emotion_speaker1[length1-1]==0) or (emotion_speaker1[length1-1]==1) or (emotion_speaker1[length1-1]==2) or (emotion_speaker1[length1-1]==3) or (emotion_speaker1[length1-1]==5)) and ((emotion_speaker2[length2-1]==0) or emotion_speaker2[length2-1]==1 or emotion_speaker2[length2-1]==2 or emotion_speaker2[length2-1]==3 or emotion_speaker2[length2-1]==5)):\n",
    "            hypo2_q_ini.add(int(speakera.id))\n",
    "len(hypo2_t_ini)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "475 0.5663157894736842\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= not negative emotion for both speakers\n",
    "#final= negative emotion of any one speaker\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=False):\n",
    "    if(round(score,5)==-0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    if (emotion_speaker1[0]==0 or emotion_speaker1[0]==4 or emotion_speaker1[0]==6) and (emotion_speaker2[0]==0 or emotion_speaker2[0]==4 or emotion_speaker2[0]==6):\n",
    "        hypo2_t_ini.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    if length1>1:\n",
    "        if ((emotion_speaker1[0]==0 or emotion_speaker1[0]==4 or emotion_speaker1[0]==6) and (emotion_speaker2[0]==0 or emotion_speaker2[0]==4 or emotion_speaker2[0]==6)) and(((emotion_speaker1[length1-1]==0) or (emotion_speaker1[length1-1]==1) or (emotion_speaker1[length1-1]==2) or (emotion_speaker1[length1-1]==3) or (emotion_speaker1[length1-1]==5)) and ((emotion_speaker2[length2-1]==0) or emotion_speaker2[length2-1]==1 or emotion_speaker2[length2-1]==2 or emotion_speaker2[length2-1]==3 or emotion_speaker2[length2-1]==5)):\n",
    "            hypo2_q_ini.add(int(speakerb.id))\n",
    "print(len(hypo2_t_ini),len(hypo2_q_ini)/len(hypo2_t_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of both 2 speakers is negative or no emotion and whose initial emotions are positive or no emotion are  56.63157894736842\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of both 2 speakers is negative or no emotion and whose initial emotions are positive or no emotion are \",100*len(hypo2_q_ini)/len(hypo2_t_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chart(a_scores, b_scores, a_description, b_description, a_color=\"b\", b_color=\"g\"):\n",
    "    # get scores by marker and on aggregate\n",
    "    a_score_by_marker = a_scores[\"marker_agg2\"]\n",
    "    a_agg1, a_agg2, a_agg3 = a_scores[\"agg1\"], a_scores[\"agg2\"], a_scores[\"agg3\"]\n",
    "    b_score_by_marker = b_scores[\"marker_agg2\"]\n",
    "    b_agg1, b_agg2, b_agg3 = b_scores[\"agg1\"], b_scores[\"agg2\"], b_scores[\"agg3\"]\n",
    "\n",
    "    # the rest plots this data as a double bar graph\n",
    "    a_data_points = sorted(a_score_by_marker.items())\n",
    "    b_data_points = sorted(b_score_by_marker.items())\n",
    "    a_data_points, b_data_points = zip(*sorted(zip(a_data_points, b_data_points),\n",
    "        key=lambda x: x[0][1], reverse=True))\n",
    "    labels, a_data_points = zip(*a_data_points)\n",
    "    _, b_data_points = zip(*b_data_points)\n",
    "\n",
    "    labels = [\"aggregate 1\", \"aggregate 2\", \"aggregate 3\"] + list(labels)\n",
    "    a_data_points = [a_agg1, a_agg2, a_agg3] + list(a_data_points)\n",
    "    b_data_points = [b_agg1, b_agg2, b_agg3] + list(b_data_points)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticks(np.arange(len(a_data_points)) + 0.35)\n",
    "    ax.set_xticklabels(labels, rotation=\"vertical\")\n",
    "\n",
    "    ax.bar(np.arange(len(a_data_points)), a_data_points, 0.35, color=a_color)\n",
    "    ax.bar(np.arange(len(b_data_points)) + 0.35, b_data_points, 0.35, color=b_color)\n",
    "\n",
    "    b_patch = mpatches.Patch(color=\"b\",\n",
    "                             label=a_description + \" (total: \" +\n",
    "                             str(a_scores[\"count_agg1\"]) + \", \" +\n",
    "                             str(a_scores[\"count_agg2\"]) + \")\")\n",
    "    g_patch = mpatches.Patch(color=\"g\",\n",
    "                             label=b_description + \" (total: \"  +\n",
    "                             str(b_scores[\"count_agg1\"]) + \", \" +\n",
    "                             str(b_scores[\"count_agg2\"]) + \")\")\n",
    "    plt.legend(handles=[b_patch, g_patch])\n",
    "\n",
    "    filename = str(a_description) + \" vs \" + str(b_description) + \".png\"\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    print('Created chart \"' + filename + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
