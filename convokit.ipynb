{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from convokit import Corpus, Speaker, Utterance\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the directory with where your downloaded cornell movie dialogs corpus is saved\n",
    "data_dir = \"C:\\\\Users\\\\Vinay\\\\OneDrive\\\\Documents\\\\GitHub\\\\Coordination\\\\ijcnlp_dailydialog\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"dialogues_text.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    dialog_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13118/13118 [00:00<00:00, 191236.62it/s]\n"
     ]
    }
   ],
   "source": [
    "utterance_data=[]\n",
    "count=0\n",
    "for utterance in tqdm(dialog_data):\n",
    "    \n",
    "    utterance_data.append( [info.strip( ) for info in utterance.split(\" __eou__\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(utterance_data)):\n",
    "    utterance_data[i].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding utterance index for a dialog\n",
    "x=0\n",
    "for i in range(len(utterance_data)):\n",
    "    for j in range(len(utterance_data[i])):\n",
    "        utterance_data[i][j]+=\" __alp__ \"+(str(j+x))\n",
    "    x+=len(utterance_data[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"dialogues_act.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    dialog_act = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13118/13118 [00:00<00:00, 536768.12it/s]\n"
     ]
    }
   ],
   "source": [
    "utterance_act=[]\n",
    "for utterance in tqdm(dialog_act):\n",
    "    \n",
    "    utterance_act.append([info.strip() for info in utterance.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(utterance_data)):\n",
    "    for j in range(len(utterance_data[i])):\n",
    "        utterance_data[i][j]+=\" __alp__ \"+utterance_act[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"dialogues_emotion.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    dialog_emotion = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13118/13118 [00:00<00:00, 502896.31it/s]\n"
     ]
    }
   ],
   "source": [
    "utterance_emotion=[]\n",
    "for utterance in tqdm(dialog_emotion):\n",
    "    \n",
    "    utterance_emotion .append( [info.strip() for info in utterance.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emotion index\n",
    "for i in range(len(utterance_data)):\n",
    "    for j in range(len(utterance_data[i])):\n",
    "        utterance_data[i][j]+=\" __alp__ \"+utterance_emotion[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So Dick , how about getting some coffee for tonight ? __alp__ 2 __alp__ 3 __alp__ 4',\n",
       " 'Coffee ? I don ’ t honestly like that kind of stuff . __alp__ 3 __alp__ 4 __alp__ 2',\n",
       " 'Come on , you can at least try a little , besides your cigarette . __alp__ 4 __alp__ 3 __alp__ 0',\n",
       " 'What ’ s wrong with that ? Cigarette is the thing I go crazy for . __alp__ 5 __alp__ 1 __alp__ 1',\n",
       " 'Not for me , Dick . __alp__ 6 __alp__ 1 __alp__ 0']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So Dick , how about getting some coffee for tonight ? __alp__ 2 __alp__ 3 __alp__ 4 __alp__ 2',\n",
       " 'Coffee ? I don ’ t honestly like that kind of stuff . __alp__ 3 __alp__ 4 __alp__ 2 __alp__ 3',\n",
       " 'Come on , you can at least try a little , besides your cigarette . __alp__ 4 __alp__ 3 __alp__ 0 __alp__ 2',\n",
       " 'What ’ s wrong with that ? Cigarette is the thing I go crazy for . __alp__ 5 __alp__ 1 __alp__ 1 __alp__ 3',\n",
       " 'Not for me , Dick . __alp__ 6 __alp__ 1 __alp__ 0 __alp__ 2']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding speakers\n",
    "#we will assume speaker 0 and 1 speaking alternatively\n",
    "for i in range(len(utterance_data)):\n",
    "    for j in range(len(utterance_data[i])):\n",
    "        utterance_data[i][j]+=\" __alp__ \"+str(2*i+j%2)\n",
    "utterance_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversation id\n",
    "for i in range(len(utterance_data)):\n",
    "    for j in range(len(utterance_data[i])):\n",
    "        utterance_data[i][j]+=\" __alp__ \"+str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So Dick , how about getting some coffee for tonight ? __alp__ 2 __alp__ 3 __alp__ 4 __alp__ 2 __alp__ 1',\n",
       " 'Coffee ? I don ’ t honestly like that kind of stuff . __alp__ 3 __alp__ 4 __alp__ 2 __alp__ 3 __alp__ 1',\n",
       " 'Come on , you can at least try a little , besides your cigarette . __alp__ 4 __alp__ 3 __alp__ 0 __alp__ 2 __alp__ 1',\n",
       " 'What ’ s wrong with that ? Cigarette is the thing I go crazy for . __alp__ 5 __alp__ 1 __alp__ 1 __alp__ 3 __alp__ 1',\n",
       " 'Not for me , Dick . __alp__ 6 __alp__ 1 __alp__ 0 __alp__ 2 __alp__ 1']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ok , I'll get this rate right away . __alp__ 102978 __alp__ 4 __alp__ 0 __alp__ 26235 __alp__ 13117\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_data_new=[]\n",
    "for xs in utterance_data:\n",
    "    for x in xs:\n",
    "        utterance_data_new.append(x)\n",
    "utterance_data_new[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_meta={}\n",
    "for i in range(26236):\n",
    "    speaker_meta[str(i)]={\"speaker_id\": i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_speakers = {k: Speaker(id = k, meta = v) for k,v in speaker_meta.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102980/102980 [00:00<00:00, 132514.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of utterances = 102980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "utterance_corpus = {}\n",
    "count = 0\n",
    "for utterance in tqdm(utterance_data_new):\n",
    "    \n",
    "    utterance_info = [info.strip() for info in utterance.split(\" __alp__\")]\n",
    "    \n",
    "    if len(utterance_info) < 4:\n",
    "        print(utterance_info)\n",
    "        \n",
    "    try:\n",
    "        text, utt_idx, utt_act, utt_emo,speaker_id,conver_id = utterance_info[0], utterance_info[1], utterance_info[2], int(utterance_info[3]),utterance_info[4],utterance_info[5]\n",
    "    except:\n",
    "        print(utterance_info)\n",
    "    \n",
    "    meta = {'utt_emo': utt_emo, 'utt_idx': utt_idx,'utt_act': utt_act}\n",
    "    \n",
    "    # root & reply_to will be updated later, timestamp is not applicable \n",
    "    utterance_corpus[utt_idx] = Utterance(id=utt_idx, speaker= corpus_speakers[speaker_id], text=text, conversation_id=conver_id, meta= meta)\n",
    "\n",
    "print(\"Total number of utterances = {}\".format(len(utterance_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'obj_type': 'utterance', 'meta': {'utt_emo': 2, 'utt_idx': '0', 'utt_act': '3'}, 'vectors': [], 'speaker': Speaker({'obj_type': 'speaker', 'meta': {'speaker_id': 0}, 'vectors': [], 'owner': None, 'id': '0'}), 'conversation_id': '0', 'reply_to': None, 'timestamp': None, 'text': 'The kitchen stinks .', 'owner': None, 'id': '0'})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_corpus['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102980"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(utterance_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_id_definer=0\n",
    "for i in range(len(utterance_corpus)):\n",
    "    if i==0 or utterance_corpus[str(i)].conversation_id!=utterance_corpus[str(i-1)].conversation_id:\n",
    "        utterance_corpus[str(i)].reply_to=None\n",
    "    else:\n",
    "        utterance_corpus[str(i)].reply_to=str(i-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance(id: '20', conversation_id: 5, reply-to: 19, speaker: Speaker(id: 10, vectors: [], meta: {'speaker_id': 10}), timestamp: None, text: 'What ’ s wrong ? Didn ’ t you think it was fun ? !', vectors: [], meta: {'utt_emo': 6, 'utt_idx': '20', 'utt_act': '2'})\n",
      "10\n",
      "{'speaker_id': 21}\n"
     ]
    }
   ],
   "source": [
    "print(utterance_corpus['20'])\n",
    "print(utterance_corpus['50'].conversation_id)\n",
    "print(utterance_corpus['50'].speaker.meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "utternace_list=utterance_corpus.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordination_corpus=Corpus(utterances=utternace_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations in the dataset = 13118\n"
     ]
    }
   ],
   "source": [
    "print(\"number of conversations in the dataset = {}\".format(len(coordination_corpus.get_conversation_ids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample conversation 0:\n",
      "['0', '1']\n",
      "sample conversation 1:\n",
      "['2', '3', '4', '5', '6']\n",
      "sample conversation 2:\n",
      "['7', '8', '9', '10']\n",
      "sample conversation 3:\n",
      "['11', '12', '13', '14']\n"
     ]
    }
   ],
   "source": [
    "convo_ids = coordination_corpus.get_conversation_ids()\n",
    "for i, convo_idx in enumerate(convo_ids[0:4]):\n",
    "    print(\"sample conversation {}:\".format(i))\n",
    "    print(coordination_corpus.get_conversation(convo_idx).get_utterance_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordination_corpus.dump(\"coordination-corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import meta_index\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterances-index': {'utt_emo': [\"<class 'int'>\"],\n",
       "  'utt_idx': [\"<class 'str'>\"],\n",
       "  'utt_act': [\"<class 'str'>\"]},\n",
       " 'speakers-index': {'speaker_id': [\"<class 'int'>\"]},\n",
       " 'conversations-index': {},\n",
       " 'overall-index': {},\n",
       " 'version': 1,\n",
       " 'vectors': []}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_index(filename = os.path.join(os.path.expanduser(\"~\"), \".convokit/saved-corpora/coordination-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import convokit\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= Corpus(utterances=utternace_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<convokit.model.corpus.Corpus object at 0x000002EABF2C8C40>\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord= convokit.Coordination()\n",
    "coord.fit(corpus)\n",
    "coord.transform(corpus)\n",
    "\n",
    "speaker_a= lambda speaker: int(speaker.meta[\"speaker_id\"])%2==0\n",
    "speaker_b= lambda speaker: int(speaker.meta[\"speaker_id\"])%2==1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_of_emotions={}\n",
    "list_of_emotion=[]\n",
    "for i in range(len(utterance_corpus)):\n",
    "    emotion_key=utterance_corpus[str(i)].speaker.meta['speaker_id']\n",
    "    emotion_dict=utterance_corpus[str(i)].meta['utt_emo']\n",
    "    if emotion_key in dictionary_of_emotions:\n",
    "        dictionary_of_emotions[emotion_key].append(emotion_dict)\n",
    "    else: \n",
    "        dictionary_of_emotions[emotion_key]=[emotion_dict]\n",
    "for i in range(26236):\n",
    "    list_of_emotion.append(dictionary_of_emotions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= not equal\n",
    "#final=equal\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "t=set()\n",
    "q=set()\n",
    "initial_emotions=[0,0,0,0,0,0,0]\n",
    "final_emotion_same=[0,0,0,0,0,0,0]\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "        t.add(int(speakera.id))\n",
    "        initial_emotions[emotion_speaker1[0]]+=1\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (((emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0))):\n",
    "        #t.discard(int(speakera.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]!=emotion_speaker2[0] and emotion_speaker1[length1-1]==emotion_speaker2[length2-1]:\n",
    "            q.add(int(speakera.id))\n",
    "            final_emotion_same[emotion_speaker1[0]]+=1\n",
    "        #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (((emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0))):\n",
    "            #q.discard(int(speakera.id))\n",
    "len(t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "115 0.4\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= not equal\n",
    "#final= equal\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "        t.add(int(speakerb.id))\n",
    "        initial_emotions[emotion_speaker2[0]]+=1\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "   # if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #t.discard(int(speakerb.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]!=emotion_speaker2[0] and emotion_speaker1[length1-1]==emotion_speaker2[length2-1]:\n",
    "            q.add(int(speakerb.id))\n",
    "            final_emotion_same[emotion_speaker2[0]]+=1\n",
    "        #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "            #q.discard(int(speakerb.id))\n",
    "print(len(t),len(q)/len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers whose initial emotions are not equal are  40.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers whose initial emotions are not equal are \",100*len(q)/len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= equal\n",
    "#final= equal\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "w=set()\n",
    "e=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "        w.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]==emotion_speaker2[0]) or (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #w.discard(int(speakera.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]==emotion_speaker2[0] and emotion_speaker1[length1-1]==emotion_speaker2[length2-1]:\n",
    "            e.add(int(speakera.id))\n",
    "        #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "            #e.discard(int(speakera.id))\n",
    "len(w)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "651 0.6605222734254992\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= equal\n",
    "#final= equal\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "        w.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #w.discard(int(speakerb.id))\n",
    "\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]==emotion_speaker2[0] and emotion_speaker1[length1-1]==emotion_speaker2[length2-1]:\n",
    "            e.add(int(speakerb.id))\n",
    "        #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "            #e.discard(int(speakerb.id))\n",
    "print(len(w),len(e)/len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers whose initial emotions are also same are  66.05222734254993\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers whose initial emotions are also same are \",100*len(e)/len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= not equal\n",
    "#final= similar\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "similar_t=set()\n",
    "similar_q=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "        similar_t.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #similar_t.discard(int(speakera.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "            if emotion_speaker1[length1-1]==emotion_speaker2[length2-1] or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==4 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==4) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==3) :\n",
    "                similar_q.add(int(speakera.id))\n",
    "            #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "                #similar_q.discard(int(speakera.id))\n",
    "len(similar_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "115 0.41739130434782606\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= not equal\n",
    "#final= similar\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "        similar_t.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #similar_t.discard(int(speakerb.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]!=emotion_speaker2[0]:\n",
    "            if emotion_speaker1[length1-1]==emotion_speaker2[length2-1] or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==4 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==4) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==3)  :\n",
    "                similar_q.add(int(speakerb.id))\n",
    "            #if (emotion_speaker1[0]!=emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "                #similar_q.discard(int(speakerb.id))\n",
    "print(len(similar_t),len(similar_q)/len(similar_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers is similar whose initial emotions are not same are are  41.73913043478261\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers is similar whose initial emotions are not same are are \",100*len(similar_q)/len(similar_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= equal\n",
    "#final= similar\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "similar_t_equal=set()\n",
    "similar_q_equal=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "        similar_t_equal.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #similar_t_equal.discard(int(speakera.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "            if emotion_speaker1[length1-1]==emotion_speaker2[length2-1] or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==4 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==4) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==3) :\n",
    "                similar_q_equal.add(int(speakera.id))\n",
    "            #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "                #similar_q_equal.discard(int(speakera.id))\n",
    "len(similar_t_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "651 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= equal\n",
    "#final= similar\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=True):\n",
    "    if(round(score,5)==0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "        similar_t_equal.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "        #similar_t_equal.discard(int(speakerb.id))\n",
    "    if length1>1:\n",
    "        if emotion_speaker1[0]==emotion_speaker2[0]:\n",
    "            if emotion_speaker1[length1-1]==emotion_speaker2[length2-1] or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==2 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==2) or (emotion_speaker1[length1-1]==4 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==4) or (emotion_speaker1[length1-1]==6 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==6) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==3) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==5) or (emotion_speaker1[length1-1]==5 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==3 and emotion_speaker2[length2-1]==1) or (emotion_speaker1[length1-1]==1 and emotion_speaker2[length2-1]==3)  :\n",
    "                similar_q_equal.add(int(speakerb.id))\n",
    "            #if (emotion_speaker1[0]==emotion_speaker2[0]) and (emotion_speaker1[0]==0 and emotion_speaker1[length1-1]==0) or (emotion_speaker2[0]==0 and emotion_speaker2[length2-1]==0):\n",
    "                #similar_q_equal.discard(int(speakerb.id))\n",
    "print(len(similar_t_equal),len(similar_q_equal)/len(similar_t_equal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers is similar whose initial emotions are also same are are  66.66666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of 2 speakers is similar whose initial emotions are also same are are \",100*len(similar_q_equal)/len(similar_t_equal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= any emotion\n",
    "#final= negative emotion of any one speaker\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "hypo2_t=set()\n",
    "hypo2_q=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=False):\n",
    "    if(round(score,5)==-0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    hypo2_t.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    if length1>1:\n",
    "        if ((emotion_speaker1[length1-1]==0) or (emotion_speaker1[length1-1]==1) or (emotion_speaker1[length1-1]==2) or (emotion_speaker1[length1-1]==3) or (emotion_speaker1[length1-1]==5)) and ((emotion_speaker2[length2-1]==0) or emotion_speaker2[length2-1]==1 or emotion_speaker2[length2-1]==2 or emotion_speaker2[length2-1]==3 or emotion_speaker2[length2-1]==5):\n",
    "            hypo2_q.add(int(speakera.id))\n",
    "len(hypo2_t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "496 0.5705645161290323\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= anything\n",
    "#final= negative emotion of any one speaker\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=False):\n",
    "    if(round(score,5)==-0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    hypo2_t.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    if length1>1:\n",
    "        if (emotion_speaker1[length1-1]==0 or emotion_speaker1[length1-1]==1 or emotion_speaker1[length1-1]==2 or emotion_speaker1[length1-1]==3 or emotion_speaker1[length1-1]==5) and (emotion_speaker2[length2-1]==0 or emotion_speaker2[length2-1]==1 or emotion_speaker2[length2-1]==2 or emotion_speaker2[length2-1]==3 or emotion_speaker2[length2-1]==5):\n",
    "            hypo2_q.add(int(speakerb.id))\n",
    "print(len(hypo2_t),len(hypo2_q)/len(hypo2_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of both 2 speakers is negative or no emotion are  57.056451612903224\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of both 2 speakers is negative or no emotion are \",100*len(hypo2_q)/len(hypo2_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a to b\n",
    "#initial= not negative emotion for both speakers\n",
    "#final= negative emotion of any one speaker\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "hypo2_t_ini=set()\n",
    "hypo2_q_ini=set()\n",
    "speakera_to_speakerb=coord.summarize(corpus, speaker_a, speaker_b)\n",
    "for speakera, score in sorted(speakera_to_speakerb.averages_by_speaker().items(),\n",
    "    key=lambda x: x[1], reverse=False):\n",
    "    if(round(score,5)==-0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakera.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakera.id)-1]\n",
    "    if (emotion_speaker1[0]==0 or emotion_speaker1[0]==4 or emotion_speaker1[0]==6) and (emotion_speaker2[0]==0 or emotion_speaker2[0]==4 or emotion_speaker2[0]==6):\n",
    "        hypo2_t_ini.add(int(speakera.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    if length1>1:\n",
    "        if ((emotion_speaker1[0]==0 or emotion_speaker1[0]==4 or emotion_speaker1[0]==6) and (emotion_speaker2[0]==0 or emotion_speaker2[0]==4 or emotion_speaker2[0]==6)) and (((emotion_speaker1[length1-1]==0) or (emotion_speaker1[length1-1]==1) or (emotion_speaker1[length1-1]==2) or (emotion_speaker1[length1-1]==3) or (emotion_speaker1[length1-1]==5)) and ((emotion_speaker2[length2-1]==0) or emotion_speaker2[length2-1]==1 or emotion_speaker2[length2-1]==2 or emotion_speaker2[length2-1]==3 or emotion_speaker2[length2-1]==5)):\n",
    "            hypo2_q_ini.add(int(speakera.id))\n",
    "len(hypo2_t_ini)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:\n",
      "475 0.5663157894736842\n"
     ]
    }
   ],
   "source": [
    "#b to a \n",
    "#initial= not negative emotion for both speakers\n",
    "#final= negative emotion of any one speaker\n",
    "print('Eventual emotion is same by what ratio if coordination of any one of the speakers is greater then 0.1 towards the other speaker:')\n",
    "speakerb_to_speakera=coord.summarize(corpus, speaker_b, speaker_a)\n",
    "for speakerb, score in sorted(speakerb_to_speakera.averages_by_speaker().items(),key=lambda x: x[1], reverse=False):\n",
    "    if(round(score,5)==-0.1):\n",
    "        break\n",
    "    if int(speakera.id)%2==0:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)+1]\n",
    "    else:\n",
    "        emotion_speaker1=list_of_emotion[int(speakerb.id)]\n",
    "        emotion_speaker2=list_of_emotion[int(speakerb.id)-1]\n",
    "    if (emotion_speaker1[0]==0 or emotion_speaker1[0]==4 or emotion_speaker1[0]==6) and (emotion_speaker2[0]==0 or emotion_speaker2[0]==4 or emotion_speaker2[0]==6):\n",
    "        hypo2_t_ini.add(int(speakerb.id))\n",
    "    length1=len(emotion_speaker1)\n",
    "    length2=len(emotion_speaker2)\n",
    "    if length1>1:\n",
    "        if ((emotion_speaker1[0]==0 or emotion_speaker1[0]==4 or emotion_speaker1[0]==6) and (emotion_speaker2[0]==0 or emotion_speaker2[0]==4 or emotion_speaker2[0]==6)) and(((emotion_speaker1[length1-1]==0) or (emotion_speaker1[length1-1]==1) or (emotion_speaker1[length1-1]==2) or (emotion_speaker1[length1-1]==3) or (emotion_speaker1[length1-1]==5)) and ((emotion_speaker2[length2-1]==0) or emotion_speaker2[length2-1]==1 or emotion_speaker2[length2-1]==2 or emotion_speaker2[length2-1]==3 or emotion_speaker2[length2-1]==5)):\n",
    "            hypo2_q_ini.add(int(speakerb.id))\n",
    "print(len(hypo2_t_ini),len(hypo2_q_ini)/len(hypo2_t_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the total number percentage of cases in which the eventual(final) emotion of both 2 speakers is negative or no emotion and whose initial emotions are positive or no emotion are  56.63157894736842\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the total number percentage of cases in which the eventual(final) emotion of both 2 speakers is negative or no emotion and whose initial emotions are positive or no emotion are \",100*len(hypo2_q_ini)/len(hypo2_t_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chart(a_scores, b_scores, a_description, b_description, a_color=\"b\", b_color=\"g\"):\n",
    "    # get scores by marker and on aggregate\n",
    "    a_score_by_marker = a_scores[\"marker_agg2\"]\n",
    "    a_agg1, a_agg2, a_agg3 = a_scores[\"agg1\"], a_scores[\"agg2\"], a_scores[\"agg3\"]\n",
    "    b_score_by_marker = b_scores[\"marker_agg2\"]\n",
    "    b_agg1, b_agg2, b_agg3 = b_scores[\"agg1\"], b_scores[\"agg2\"], b_scores[\"agg3\"]\n",
    "\n",
    "    # the rest plots this data as a double bar graph\n",
    "    a_data_points = sorted(a_score_by_marker.items())\n",
    "    b_data_points = sorted(b_score_by_marker.items())\n",
    "    a_data_points, b_data_points = zip(*sorted(zip(a_data_points, b_data_points),\n",
    "        key=lambda x: x[0][1], reverse=True))\n",
    "    labels, a_data_points = zip(*a_data_points)\n",
    "    _, b_data_points = zip(*b_data_points)\n",
    "\n",
    "    labels = [\"aggregate 1\", \"aggregate 2\", \"aggregate 3\"] + list(labels)\n",
    "    a_data_points = [a_agg1, a_agg2, a_agg3] + list(a_data_points)\n",
    "    b_data_points = [b_agg1, b_agg2, b_agg3] + list(b_data_points)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticks(np.arange(len(a_data_points)) + 0.35)\n",
    "    ax.set_xticklabels(labels, rotation=\"vertical\")\n",
    "\n",
    "    ax.bar(np.arange(len(a_data_points)), a_data_points, 0.35, color=a_color)\n",
    "    ax.bar(np.arange(len(b_data_points)) + 0.35, b_data_points, 0.35, color=b_color)\n",
    "\n",
    "    b_patch = mpatches.Patch(color=\"b\",\n",
    "                             label=a_description + \" (total: \" +\n",
    "                             str(a_scores[\"count_agg1\"]) + \", \" +\n",
    "                             str(a_scores[\"count_agg2\"]) + \")\")\n",
    "    g_patch = mpatches.Patch(color=\"g\",\n",
    "                             label=b_description + \" (total: \"  +\n",
    "                             str(b_scores[\"count_agg1\"]) + \", \" +\n",
    "                             str(b_scores[\"count_agg2\"]) + \")\")\n",
    "    plt.legend(handles=[b_patch, g_patch])\n",
    "\n",
    "    filename = str(a_description) + \" vs \" + str(b_description) + \".png\"\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    print('Created chart \"' + filename + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "labels = ['No-emotion', 'Anger', 'Disgust', 'Fear', 'happiness','Sadness','Surprise']\n",
    "\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, initial_emotions, width, label='Initial emotion of the Speaker with higher coordination')\n",
    "rects2 = ax.bar(x + width/2, final_emotion_same, width, label='Number of cases in a particular emotion that give same emotion')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Scores by group and gender')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
